{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Running on: NVIDIA A100 80GB PCIe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvdoninav\u001b[0m (\u001b[33mvdoninav-hse\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cw25/wandb/run-20250417_231912-5p7yawat</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vdoninav-hse/coursework_2025/runs/5p7yawat' target=\"_blank\">d8</a></strong> to <a href='https://wandb.ai/vdoninav-hse/coursework_2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vdoninav-hse/coursework_2025' target=\"_blank\">https://wandb.ai/vdoninav-hse/coursework_2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vdoninav-hse/coursework_2025/runs/5p7yawat' target=\"_blank\">https://wandb.ai/vdoninav-hse/coursework_2025/runs/5p7yawat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 4.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 3.7744, BERT-P: 0.6696, R: 0.6081, F1: 0.6360\n",
      "\n",
      "===== Epoch 2/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 2):  73%|████████   | 1490/2037 [07:28<02:45,  3.30it/s, batch_loss=4.4137]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.9371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 3.1015, BERT-P: 0.6684, R: 0.6379, F1: 0.6521\n",
      "\n",
      "===== Epoch 4/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.9644, BERT-P: 0.6401, R: 0.6381, F1: 0.6377\n",
      "\n",
      "===== Epoch 5/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.7943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.9219, BERT-P: 0.6626, R: 0.6499, F1: 0.6556\n",
      "\n",
      "===== Epoch 6/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.7540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8398, BERT-P: 0.6563, R: 0.6469, F1: 0.6508\n",
      "\n",
      "===== Epoch 7/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.7233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8169, BERT-P: 0.6605, R: 0.6528, F1: 0.6559\n",
      "\n",
      "===== Epoch 8/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 8):   3%|▎            | 51/2037 [00:15<09:55,  3.34it/s, batch_loss=3.7203]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "from usecrets import WANDB_API_KEY\n",
    "from lstm import BiLSTMSeq2SeqMHA_Residual\n",
    "from distill import collate_fn, preprocess_function, train_distillation\n",
    "\n",
    "\n",
    "from config_distill_d6 import (\n",
    "    TEACHER_MODEL_NAME,\n",
    "    MAX_SOURCE_LEN,\n",
    "    MAX_TARGET_LEN,\n",
    "    EMBED_DIM,\n",
    "    ENC_HIDDEN_DIM,\n",
    "    DEC_HIDDEN_DIM,\n",
    "    NUM_LAYERS,\n",
    "    DROPOUT,\n",
    "    MHA_NUM_HEADS,\n",
    "    BATCH_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    NUM_EPOCHS,\n",
    "    TEMPERATURE,\n",
    "    BEAM_SIZE,\n",
    "    BEAM_MAX_LENGTH,\n",
    "    WANDB_PROJECT,\n",
    "    WANDB_RUN_NAME,\n",
    "    MIN_LEN,\n",
    ")\n",
    "\n",
    "\n",
    "# ====================== CONFIG / CONSTANTS ======================\n",
    "seed = 52\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Running on: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "\n",
    "\n",
    "# ====================== MAIN ======================\n",
    "def main():\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=WANDB_RUN_NAME,\n",
    "        config={\n",
    "            \"num_train_epochs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"beam_size\": BEAM_SIZE,\n",
    "            \"teacher\": TEACHER_MODEL_NAME,\n",
    "            \"embed_dim\": EMBED_DIM,\n",
    "            \"ENC_HIDDEN_DIM\": ENC_HIDDEN_DIM,\n",
    "            \"DEC_HIDDEN_DIM\": DEC_HIDDEN_DIM,\n",
    "            \"NUM_LAYERS\": NUM_LAYERS,\n",
    "            \"DROPOUT\": DROPOUT,\n",
    "            \"MHA_NUM_HEADS\": MHA_NUM_HEADS,\n",
    "            \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    teacher_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        TEACHER_MODEL_NAME\n",
    "    ).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME)\n",
    "\n",
    "    tokenizer.src_lang = \"ru_RU\"\n",
    "    tokenizer.tgt_lang = \"ru_RU\"\n",
    "\n",
    "    dataset = load_dataset(\"json\", data_files=\"train_smart.jsonl\")[\"train\"]\n",
    "    split_data = dataset.train_test_split(test_size=0.025, seed=52)\n",
    "    train_raw = split_data[\"train\"]\n",
    "    val_raw = split_data[\"test\"]\n",
    "\n",
    "    train_ds = train_raw.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, MAX_SOURCE_LEN, MAX_TARGET_LEN),\n",
    "        batched=False,\n",
    "    )\n",
    "    val_ds = val_raw.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, MAX_SOURCE_LEN, MAX_TARGET_LEN),\n",
    "        batched=False,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "    student_model = BiLSTMSeq2SeqMHA_Residual(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        enc_hidden_dim=ENC_HIDDEN_DIM,\n",
    "        dec_hidden_dim=DEC_HIDDEN_DIM,\n",
    "        pad_idx=tokenizer.pad_token_id,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "        num_heads=MHA_NUM_HEADS,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    train_distillation(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        temperature=TEMPERATURE,\n",
    "        device=DEVICE,\n",
    "        wandb_run_name=WANDB_RUN_NAME\n",
    "    )\n",
    "\n",
    "    wandb.watch(student_model, log=\"all\")\n",
    "\n",
    "    torch.save(\n",
    "        student_model.state_dict(), f\"students/{WANDB_RUN_NAME}/student_model.pt\"\n",
    "    )\n",
    "    tokenizer.save_pretrained(f\"students/{WANDB_RUN_NAME}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
