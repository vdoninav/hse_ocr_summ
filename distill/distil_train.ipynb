{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Running on: NVIDIA A100 80GB PCIe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvdoninav\u001b[0m (\u001b[33mvdoninav-hse\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/cw25/wandb/run-20250423_163405-vehiel2d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vdoninav-hse/coursework_2025/runs/vehiel2d' target=\"_blank\">v3_2</a></strong> to <a href='https://wandb.ai/vdoninav-hse/coursework_2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vdoninav-hse/coursework_2025' target=\"_blank\">https://wandb.ai/vdoninav-hse/coursework_2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vdoninav-hse/coursework_2025/runs/vehiel2d' target=\"_blank\">https://wandb.ai/vdoninav-hse/coursework_2025/runs/vehiel2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 4.7004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 3.6556, BERT-P: 0.6618, R: 0.6171, F1: 0.6380\n",
      "\n",
      "===== Epoch 2/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 4.2218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 3.2987, BERT-P: 0.5599, R: 0.6132, F1: 0.5839\n",
      "\n",
      "===== Epoch 3/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 3):  53%|█████▊     | 1084/2037 [06:47<03:55,  4.04it/s, batch_loss=4.3772]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 3.0904, BERT-P: 0.6336, R: 0.6349, F1: 0.6338\n",
      "\n",
      "===== Epoch 5/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 4.0087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 3.0453, BERT-P: 0.5924, R: 0.6188, F1: 0.6042\n",
      "\n",
      "===== Epoch 6/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.9760, BERT-P: 0.6197, R: 0.6263, F1: 0.6225\n",
      "\n",
      "===== Epoch 7/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 7):  72%|███████▉   | 1473/2037 [06:36<04:01,  2.34it/s, batch_loss=3.4398]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.9248, BERT-P: 0.6314, R: 0.6369, F1: 0.6336\n",
      "\n",
      "===== Epoch 10/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.9222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.9190, BERT-P: 0.6364, R: 0.6332, F1: 0.6344\n",
      "\n",
      "===== Epoch 11/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.9143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.9067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8760, BERT-P: 0.6602, R: 0.6494, F1: 0.6544\n",
      "\n",
      "===== Epoch 13/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8634, BERT-P: 0.6585, R: 0.6518, F1: 0.6549\n",
      "\n",
      "===== Epoch 14/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8812, BERT-P: 0.6471, R: 0.6387, F1: 0.6425\n",
      "\n",
      "===== Epoch 15/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8606, BERT-P: 0.6610, R: 0.6570, F1: 0.6586\n",
      "\n",
      "===== Epoch 16/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8477, BERT-P: 0.6575, R: 0.6520, F1: 0.6543\n",
      "\n",
      "===== Epoch 17/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8511, BERT-P: 0.6619, R: 0.6530, F1: 0.6571\n",
      "\n",
      "===== Epoch 18/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.8226, BERT-P: 0.6647, R: 0.6574, F1: 0.6607\n",
      "\n",
      "===== Epoch 19/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 19):  98%|█████████▊| 1998/2037 [13:01<00:09,  4.05it/s, batch_loss=3.8169]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training (epoch 30):  99%|█████████▉| 2020/2037 [14:24<00:07,  2.32it/s, batch_loss=3.3071]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training (epoch 31):  94%|█████████▍| 1924/2037 [12:29<00:48,  2.34it/s, batch_loss=4.3457]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 33):  79%|███████▉  | 1606/2037 [07:31<03:04,  2.33it/s, batch_loss=3.5025]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7883, BERT-P: 0.6791, R: 0.6672, F1: 0.6727\n",
      "\n",
      "===== Epoch 34/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 34):  73%|███████▎  | 1488/2037 [06:15<02:24,  3.81it/s, batch_loss=4.5951]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7659, BERT-P: 0.6754, R: 0.6645, F1: 0.6697\n",
      "\n",
      "===== Epoch 35/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 35):  73%|███████▎  | 1486/2037 [07:15<02:20,  3.92it/s, batch_loss=4.1672]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7755, BERT-P: 0.6801, R: 0.6656, F1: 0.6724\n",
      "\n",
      "===== Epoch 36/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7694, BERT-P: 0.6791, R: 0.6667, F1: 0.6725\n",
      "\n",
      "===== Epoch 37/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 37):  62%|██████▏   | 1268/2037 [08:55<03:15,  3.93it/s, batch_loss=4.4519]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7447, BERT-P: 0.6740, R: 0.6638, F1: 0.6685\n",
      "\n",
      "===== Epoch 39/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 39):  26%|██▊        | 525/2037 [03:42<10:39,  2.36it/s, batch_loss=3.8303]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 40):  80%|████████  | 1632/2037 [09:49<02:52,  2.35it/s, batch_loss=3.5797]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7557, BERT-P: 0.6746, R: 0.6639, F1: 0.6687\n",
      "\n",
      "===== Epoch 42/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 42):  73%|███████▎  | 1477/2037 [07:44<03:58,  2.35it/s, batch_loss=3.3946]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7593, BERT-P: 0.6794, R: 0.6670, F1: 0.6727\n",
      "\n",
      "===== Epoch 43/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7496, BERT-P: 0.6720, R: 0.6632, F1: 0.6672\n",
      "\n",
      "===== Epoch 44/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distill Loss (Train): 3.8231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT eval:  30%|████████████▉                              | 16/53 [03:21<07:25, 12.05s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7622, BERT-P: 0.6810, R: 0.6686, F1: 0.6743\n",
      "\n",
      "===== Epoch 45/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 45):  73%|███████▎  | 1489/2037 [06:08<02:14,  4.07it/s, batch_loss=4.1602]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7524, BERT-P: 0.6725, R: 0.6627, F1: 0.6671\n",
      "\n",
      "===== Epoch 46/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 46):  73%|███████▎  | 1484/2037 [06:08<02:16,  4.05it/s, batch_loss=3.3357]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7540, BERT-P: 0.6769, R: 0.6622, F1: 0.6692\n",
      "\n",
      "===== Epoch 47/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 47):  73%|███████▎  | 1487/2037 [08:10<02:15,  4.06it/s, batch_loss=3.7393]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation -- CE: 2.7503, BERT-P: 0.6852, R: 0.6672, F1: 0.6758\n",
      "\n",
      "===== Epoch 48/50 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (epoch 48):  73%|███████▎  | 1487/2037 [10:35<03:53,  2.35it/s, batch_loss=3.6604]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Training (epoch 49):  73%|███████▎  | 1493/2037 [10:38<03:53,  2.33it/s, batch_loss=3.4050]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "from usecrets import WANDB_API_KEY\n",
    "from lstm import BiLSTM\n",
    "from distill import collate_fn, preprocess_function, train_distillation\n",
    "\n",
    "\n",
    "from config_distill_v3_2 import (\n",
    "    TEACHER_MODEL_NAME,\n",
    "    MAX_SOURCE_LEN,\n",
    "    MAX_TARGET_LEN,\n",
    "    EMBED_DIM,\n",
    "    ENC_HIDDEN_DIM,\n",
    "    DEC_HIDDEN_DIM,\n",
    "    NUM_LAYERS,\n",
    "    DROPOUT,\n",
    "    BATCH_SIZE,\n",
    "    LEARNING_RATE,\n",
    "    NUM_EPOCHS,\n",
    "    TEMPERATURE,\n",
    "    BEAM_SIZE,\n",
    "    BEAM_MAX_LENGTH,\n",
    "    WANDB_PROJECT,\n",
    "    WANDB_RUN_NAME,\n",
    "    MIN_LEN,\n",
    ")\n",
    "\n",
    "\n",
    "# ====================== CONFIG / CONSTANTS ======================\n",
    "seed = 52\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Running on: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "\n",
    "\n",
    "# ====================== MAIN ======================\n",
    "def main():\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=WANDB_RUN_NAME,\n",
    "        config={\n",
    "            \"num_train_epochs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"beam_size\": BEAM_SIZE,\n",
    "            \"teacher\": TEACHER_MODEL_NAME,\n",
    "            \"embed_dim\": EMBED_DIM,\n",
    "            \"ENC_HIDDEN_DIM\": ENC_HIDDEN_DIM,\n",
    "            \"DEC_HIDDEN_DIM\": DEC_HIDDEN_DIM,\n",
    "            \"NUM_LAYERS\": NUM_LAYERS,\n",
    "            \"DROPOUT\": DROPOUT,\n",
    "            \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    teacher_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        TEACHER_MODEL_NAME\n",
    "    ).to(DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME)\n",
    "\n",
    "    tokenizer.src_lang = \"ru_RU\"\n",
    "    tokenizer.tgt_lang = \"ru_RU\"\n",
    "\n",
    "    dataset = load_dataset(\"json\", data_files=\"train_smart.jsonl\")[\"train\"]\n",
    "    split_data = dataset.train_test_split(test_size=0.025, seed=52)\n",
    "    train_raw = split_data[\"train\"]\n",
    "    val_raw = split_data[\"test\"]\n",
    "\n",
    "    train_ds = train_raw.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, MAX_SOURCE_LEN, MAX_TARGET_LEN),\n",
    "        batched=False,\n",
    "    )\n",
    "    val_ds = val_raw.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, MAX_SOURCE_LEN, MAX_TARGET_LEN),\n",
    "        batched=False,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    "    )\n",
    "\n",
    "    vocab_size = len(tokenizer)\n",
    "    student_model = BiLSTM(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        enc_hidden_dim=ENC_HIDDEN_DIM,\n",
    "        dec_hidden_dim=DEC_HIDDEN_DIM,\n",
    "        pad_idx=tokenizer.pad_token_id,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout=DROPOUT,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    train_distillation(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        lr=LEARNING_RATE,\n",
    "        temperature=TEMPERATURE,\n",
    "        device=DEVICE,\n",
    "        wandb_run_name=WANDB_RUN_NAME\n",
    "    )\n",
    "\n",
    "    wandb.watch(student_model, log=\"all\")\n",
    "\n",
    "    torch.save(\n",
    "        student_model.state_dict(), f\"students/{WANDB_RUN_NAME}/student_model.pt\"\n",
    "    )\n",
    "    tokenizer.save_pretrained(f\"students/{WANDB_RUN_NAME}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
